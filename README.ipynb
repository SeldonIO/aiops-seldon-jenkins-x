{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A practical guide to MLOps with Seldon Core and Jenkins X\n",
    "\n",
    "This tutorial provides an end-to-end tutorial that shows you how to build you MLOps pipeline with Seldon Core and Jenkins X:\n",
    "\n",
    "* Seldon Core is a machine learning deployment & orchestration engine in Kubernetes\n",
    "* Jenkins X provides automated CI+CD for Kubernetes with Preview Environments on Pull Requests\n",
    "\n",
    "## Current limitations\n",
    "\n",
    "### Prepackaged Model Servers vs Language Wrappers\n",
    "\n",
    "Ideally we would be able to show in this tutorial two workflows: 1) creating a model server, and 2) deploying multiple applications with that model server. At this point in time, there is only one workflow in this repository, which consists of building and deploying a single Seldon Model using a language wrapper, which means that the docker image is built every time. \n",
    "\n",
    "### Quickstart and buildpack\n",
    "\n",
    "A lot of the logic in this tutorial could simplified by creating a quickstart project and possibly a build pack (although most of the logic can be built as part of a build pack initially).\n",
    "\n",
    "### Docker builds in cluster required\n",
    "\n",
    "Currently, the images are built and pushed using a pod with a docker daemon, this is because of the current dependency Seldon has on S2i (Source-to-image) to build the image wrappers.\n",
    "\n",
    "Furthermore, docker is also required to run the end-to-end tests, which leverage KIND (kubernetes in docker) to spin up a test kubernetes cluster inside of a Kubernetes Pod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitive explanation\n",
    "\n",
    "In this project, we will be building an MLOps workflow to deploy your production machine learning models by buiding a re-usable pre-packaged model server through CI, and then deploying individual models using CD.\n",
    "\n",
    "[TODO: ARCHITECTURAL DIAGRAM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* A Kubernetes cluster running v1.13+ (this was run using GKE)\n",
    "* The [jx CLI](https://github.com/jenkins-x/jx/) version 2.0.916\n",
    "* Jenkins-X installed in your cluster (you can set it up with the [jx boot tutorial](https://jenkins-x.io/docs/getting-started/setup/boot/))\n",
    "* Seldon Core [v0.5.0 installed]() in your cluster\n",
    "\n",
    "Once you set everything up, we'll be ready to kick off ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up repo\n",
    "\n",
    "Now we want to start setting up our repo. For this we will create the following structure:\n",
    "\n",
    "* `jenkins-x.yml` - File specifying the CI / CD steps \n",
    "* `Makefile` - Commands to build and test model\n",
    "* `README.(md|ipynb)` - This file!\n",
    "* `VERSION` - A file containing the version which is updated upon each release\n",
    "* `charts/` - Folder containing the deployment configuration information\n",
    "* `integration/` - Folder containing integration tests using KIND\n",
    "* `src`\n",
    "    * `ModelName.py` - Model server wrapper file\n",
    "    * `test_ModelName.py` - Unit test for model server\n",
    "    * `requirements-dev.txt` - Requirements for testing\n",
    "    * `requirements.txt` - Requiremnets for prod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a model locally\n",
    "\n",
    "First we will train a machine learning model, which will help us classify news across multiple categories.\n",
    "\n",
    "### Install dependencies \n",
    "\n",
    "We will need the following dependencies in order to run the Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements-dev.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements-dev.txt\n",
    "scikit-learn==0.20.1\n",
    "pytest==5.1.1\n",
    "joblib==0.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now install the dependencies using the make command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make install_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the ML data\n",
    "\n",
    "Now that we have all the dependencies we can proceed to download the data.\n",
    "\n",
    "We will download the news stories dataset, and we'll be attempting to classify across the four classes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(\n",
    "    subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(\n",
    "    subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Printing the top 3 newstories\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Now that we've downloaded the data, we can train the ML model using a simple pipeline with basic text pre-processors and a Multiclass naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test single prediction\n",
    "\n",
    "Now that we've trained our model we can use it to predict from un-seen data.\n",
    "\n",
    "We can see below that the model is able to predict the first datapoint in the dataset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTENT:\n",
      "Subject: Re: HELP for Kidney Stones ..............\n",
      "Organization: The Avant-Garde of the Now, Ltd.\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: ucsd.edu\n",
      "\n",
      "As I recall from my bout with kidney stones, there isn't \n",
      "\n",
      "-----------\n",
      "\n",
      "PREDICTED CLASS: comp.graphics\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(f\"CONTENT:{twenty_test.data[idx][35:230]}\\n\\n-----------\\n\")\n",
    "print(f\"PREDICTED CLASS: {categories[twenty_test.target[idx]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print accuracy\n",
    "\n",
    "We can print the accuracy of the model by running the test data and counting the number of correct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print(f\"Accuracy: {np.mean(predicted == twenty_test.target):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Now we want to be able to deploy the model we just trained. For this we'll follow the standard steps to wrap the model using Seldon.\n",
    "\n",
    "### Save the trained model\n",
    "\n",
    "First we have to save the trained model in the `src/` folder, which our wrapper will load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src/model.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(text_clf, \"src/model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build wrapper that loads model\n",
    "\n",
    "Now we can actually write a simple wrapper that basically loads the model and exposes the logic through the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/SklearnServer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/SklearnServer.py\n",
    "\n",
    "import joblib, logging\n",
    "\n",
    "class SklearnServer:\n",
    "    def __init__(self):\n",
    "        self._model = joblib.load(f\"model.joblib\")\n",
    "\n",
    "    def predict(self, data, feature_names=[], metadata={}):\n",
    "        logging.info(data)\n",
    "\n",
    "        prediction = self._model.predict(data)\n",
    "\n",
    "        logging.info(prediction)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the wrapper\n",
    "\n",
    "It's best practice to write a set of unit tests to make sure that our wrapper works as expected.\n",
    "\n",
    "We'll write a single unit test and then we'll run it using Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/test_SklearnServer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/test_SklearnServer.py\n",
    "\n",
    "from .SklearnServer import SklearnServer\n",
    "import numpy as np\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "# Libraries to patch:\n",
    "import joblib\n",
    "\n",
    "EXPECTED_RESPONSE = np.array([0, 1])\n",
    "\n",
    "class FakeModel:\n",
    "    def predict(self, df):\n",
    "        return EXPECTED_RESPONSE\n",
    "\n",
    "\n",
    "@mock.patch(\"joblib.load\", return_value=FakeModel())\n",
    "def test_sklearn_server(*args, **kwargs):\n",
    "    data = [\"text 1\", \"text 2\"]\n",
    "\n",
    "    s = SklearnServer()\n",
    "    result = s.predict(data)\n",
    "    assert all(result == EXPECTED_RESPONSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: VERSION: No such file or directory\n",
      "Makefile:25: warning: overriding recipe for target 'make'\n",
      "Makefile:22: warning: ignoring old recipe for target 'make'\n",
      "pytest -s --verbose -W ignore 2>&1\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.1.1, py-1.8.0, pluggy-0.12.0 -- /home/alejandro/miniconda3/envs/reddit-classification/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/alejandro/Programming/kubernetes/seldon/sig-mlops-example\n",
      "plugins: cov-2.7.1, forked-1.0.2, localserver-0.5.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "src/test_SklearnServer.py::test_sklearn_server \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m============================== 1 passed in 1.72s ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!make test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define config files\n",
    "\n",
    "Now that our wrapper works as expected, we just need to write a set of configuration files, including dependencies and the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "scikit-learn==0.20.1\n",
    "joblib==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/seldon_model.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/seldon_model.conf\n",
    "MODEL_NAME=SklearnServer\n",
    "API_TYPE=REST\n",
    "SERVICE_TYPE=MODEL\n",
    "PERSISTENCE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container with s2i untils\n",
    "\n",
    "Now we leverage the `s2i` util to convert our wrapped model into a fully fledged REST server that exposes the logic through an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "---> Installing application source...\n",
      "---> Installing dependencies ...\n",
      "Looking in links: /whl\n",
      "Collecting scikit-learn==0.20.1 (from -r requirements.txt (line 1))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/10/26/d04320c3edf2d59b1fcd0720b46753d4d603a76e68d8ad10a9b92ab06db2/scikit_learn-0.20.1-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "Collecting joblib==0.13.2 (from -r requirements.txt (line 2))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "Collecting scipy>=0.13.3 (from scikit-learn==0.20.1->-r requirements.txt (line 1))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/29/50/a552a5aff252ae915f522e44642bb49a7b7b31677f9580cfd11bcc869976/scipy-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/site-packages (from scikit-learn==0.20.1->-r requirements.txt (line 1)) (1.17.2)\n",
      "Installing collected packages: scipy, scikit-learn, joblib\n",
      "Successfully installed joblib-0.13.2 scikit-learn-0.20.1 scipy-1.3.1\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "WARNING: You are using pip version 19.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "Build completed successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "SELDON_BASE_WRAPPER=\"seldonio/seldon-core-s2i-python36:0.12\"\n",
    "s2i build src/. $SELDON_BASE_WRAPPER sklearn-server:0.1 \\\n",
    "    --environment-file src/seldon_model.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "YOUR_DOCKER_USERNAME=\"seldonio\"\n",
    "\n",
    "docker tag sklearn-server:0.1 $YOUR_DOCKER_USERNAME/sklearn-server:0.1\n",
    "docker push $YOUR_DOCKER_USERNAME/sklearn-server:0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model \n",
    "\n",
    "Now that we've built our model, we can push it and deploy it to our kubernetes cluster for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: machinelearning.seldon.io/v1alpha2\r\n",
      "kind: SeldonDeployment\r\n",
      "metadata:\r\n",
      "  name: {{ .Values.model.name }}\r\n",
      "spec:\r\n",
      "  name: {{ .Values.model.name }}\r\n",
      "  predictors:\r\n",
      "  - name: default\r\n",
      "    graph:\r\n",
      "      name: {{ .Values.model.name }}-processor\r\n",
      "      endpoint:\r\n",
      "        type: REST\r\n",
      "      type: MODEL\r\n",
      "      children: []\r\n",
      "      parameters:\r\n",
      "      - name: model_uri\r\n",
      "        type: STRING\r\n",
      "        value: \"gs://news_classifier/model/\"\r\n",
      "    componentSpecs:\r\n",
      "    - spec:\r\n",
      "        containers:\r\n",
      "        - image: \"{{ .Values.image.respository }}:{{ .Values.image.tag }}\"\r\n",
      "          imagePullPolicy: {{ .Values.image.pullPolicy }}\r\n",
      "          name: {{ .Values.model.name }}-processor\r\n",
      "          env:\r\n",
      "{{- range $pkey, $pval := .Values.env }}\r\n",
      "          - name: {{ $pkey }}\r\n",
      "            value: {{ quote $pval }}\r\n",
      "{{- end }}\r\n",
      "        terminationGracePeriodSeconds: 1\r\n",
      "    replicas: 1\r\n",
      "    engineResources: {}\r\n",
      "    svcOrchSpec: {}\r\n",
      "    traffic: 100\r\n",
      "    explainer:\r\n",
      "      containerSpec:\r\n",
      "        name: ''\r\n",
      "        resources: {}\r\n",
      "  annotations:\r\n",
      "    seldon.io/engine-seldon-log-messages-externally: 'true'\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat charts/sklearn-model-server/templates/sklearn-seldon-deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/news-classifier-server created\r\n"
     ]
    }
   ],
   "source": [
    "!helm install charts/sklearn-model-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server by sending request\n",
    "\n",
    "Now that we've deployed our model, we can test it by sending a POST request. \n",
    "\n",
    "This can be done using our SeldonCore Python client, or by just sending it through `curl`. Both are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ndarray {\n",
       "  values {\n",
       "    number_value: 2.0\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "import numpy as np\n",
    "\n",
    "url = !kubectl get svc ambassador -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    gateway_endpoint=\"localhost:80\",\n",
    "    deployment_name=\"news-classifier-server\",\n",
    "    payload_type=\"ndarray\",\n",
    "    namespace=\"default\",\n",
    "    transport=\"rest\")\n",
    "\n",
    "response = sc.predict(data=np.array([twenty_test.data[0]]))\n",
    "\n",
    "response.response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"meta\": {\n",
      "    \"puid\": \"so6n21pkf70fm66eka28lc63cr\",\n",
      "    \"tags\": {\n",
      "    },\n",
      "    \"routing\": {\n",
      "    },\n",
      "    \"requestPath\": {\n",
      "      \"news-classifier-server-processor\": \"axsauze/sklearn-server:0.1\"\n",
      "    },\n",
      "    \"metrics\": []\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"names\": [],\n",
      "    \"ndarray\": [2.0]\n",
      "  }\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   350  100   278  100    72   7942   2057 --:--:-- --:--:-- --:--:-- 10294\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "     -d \"{'data': {'names': ['text'], 'ndarray': ['Hello world this is a test']}}\" \\\n",
    "    http://localhost/seldon/default/news-classifier-server/api/v0.1/predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"news-classifier-server\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!helm delete charts/sklearn-model-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up CI before CD\n",
    "\n",
    "We have now separated our model development into two chunks: \n",
    "\n",
    "* The first one involves the creation of a model serve, and the second one involves the CI of the model server, and the second involves the deployment of models that create the model.\n",
    "\n",
    "\n",
    "## Using the Jenkins X pipeline\n",
    "\n",
    "In order to do this we will be able to first run some tests and the push to the docker repo.\n",
    "\n",
    "For this we will be leveraging the Jenkins X file, we'll first start with a simple file that just runs the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jenkins-x.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile jenkins-x.yml\n",
    "buildPack: none\n",
    "pipelineConfig:\n",
    "  pipelines:\n",
    "    release:\n",
    "      pipeline:\n",
    "        agent:\n",
    "          image: seldonio/core-builder:0.4\n",
    "        stages:\n",
    "          - name: test-sklearn-server\n",
    "            steps:\n",
    "            - name: run-tests\n",
    "              command: make\n",
    "              args:\n",
    "              - install_dev\n",
    "              - test\n",
    "    pullRequest:\n",
    "      pipeline:\n",
    "        agent:\n",
    "          image: seldonio/core-builder:0.4\n",
    "        stages:\n",
    "          - name: test-sklearn-server\n",
    "            steps:\n",
    "            - name: run-tests\n",
    "              command: make\n",
    "              args:\n",
    "              - install_dev\n",
    "              - test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `jenkins-x.yml` file is pretty easy to understand if we read through the different steps.\n",
    "\n",
    "Basically we can define the steps of what happens upon `release` - i.e. when a PR / Commit is added to master - and what happens upon `pullRequest` - whenever someone opens a pull request.\n",
    "\n",
    "You can see that the steps are exactly the same for both release and PR for now - namely, we run `make install_dev test` which basically installs all the dependencies and runs all the tests.\n",
    "\n",
    "### Setting up the repo with the pipeline\n",
    "\n",
    "In order for the Pipeline to be executed on PR and release, we must import it into our Jenkins X cluster. \n",
    "\n",
    "We can do this by running this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jx import --no-draft=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as we import the repository into Jenkins X, the release path gets triggered.\n",
    "\n",
    "We can see the activities that have been triggered by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jx get activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And we can actually see the logs of what is happening at every step by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jx get build logs \"$GIT_USERNAME/seldon-jx-mlops/master #1 release\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As we can see, the `release` trigger is working as expected. We can now trigger the PR by opening a PR.\n",
    "\n",
    "For this, let's add a small change and push a PR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Create new branch and move into it\n",
    "git checkout -b feature-1\n",
    "\n",
    "# Add an extra space at the end\n",
    "echo \" \" >> jenkins-x.yml\n",
    "git add jenkins-x\n",
    "git commit -m \"Added extra space to trigger master\"\n",
    "git push origin feature-1\n",
    "\n",
    "# Now create pull request\n",
    "git request-pull -p origin/master ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Once we create the pull request we can visualise that the PR has been created and the bot has commented.\n",
    "\n",
    "We would now also be able to see that the tests are now running, and similar to above we can see the logs with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get build logs \"$GIT_USERNAME/seldon-jx-mlops/pr-1 #1 pr-build\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing images automatically\n",
    "Now that we're able to build some tests, we want to update the images so we can have the latest on each release.\n",
    "\n",
    "For this, we will have to add a couple of things, including:\n",
    "\n",
    "1. The task in the `jenkins-x.yml` file that would allow us to build and push the image\n",
    "2. Adding an authentication token so we can push images\n",
    "3. Mounting the docker config in the `jenkins-x.yml` to provide docker authentications (to push images)\n",
    "4. A script that starts a docker daemon and then builds+psuhes the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JX Task to Build and Push image\n",
    "\n",
    "For this, we would just have to append the following task in our jenkins file:\n",
    "    \n",
    "```\n",
    "    - name: build-and-push-images\n",
    "      command: bash\n",
    "      args:\n",
    "      - assets/scripts/build_and_push_docker_daemon.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add docker auth to your cluster\n",
    "\n",
    "Adding a docker authentication with Jenkins X can be done through a JX CLI command, which is the following:\n",
    "\n",
    "* `jx create docker auth --host https://index.docker.io/v1/ --user $YOUR_DOCKER_USERNAME --secret $YOUR_DOCKER_KEY_SECRET --email $YOUR_DOCKER_EMAIL`\n",
    "\n",
    "This comamnd will use these credentials to authenticate with Docker and create an auth token (which expires)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config to provide docker authentication\n",
    "\n",
    "This piece is slightly more extensive, as we will need to use Docker to build out containers due to the dependency on `s2i` to build the model wrappers.\n",
    "\n",
    "First we need to define the volumes that we'll be mounting to the container.\n",
    "\n",
    "The first few volumes before basically consist of the core components that docker will need to be able to run.\n",
    "```\n",
    "          volumes:\n",
    "            - name: modules\n",
    "              hostPath:\n",
    "                path: /lib/modules\n",
    "                type: Directory\n",
    "            - name: cgroup\n",
    "              hostPath:\n",
    "                path: /sys/fs/cgroup\n",
    "                type: Directory\n",
    "            - name: dind-storage\n",
    "              emptyDir: {}\n",
    "```\n",
    "We also want to mount the docker credentials which we will generate in the next step.\n",
    "```\n",
    "            - name: jenkins-docker-config-volume\n",
    "              secret:\n",
    "                items:\n",
    "                - key: config.json\n",
    "                  path: config.json\n",
    "                secretName: jenkins-docker-cfg\n",
    "```\n",
    "Once we've created the volumes, now we just need to mount them. This can be done as follows:\n",
    "```\n",
    "        options:\n",
    "          containerOptions:\n",
    "            volumeMounts:\n",
    "              - mountPath: /lib/modules\n",
    "                name: modules\n",
    "                readOnly: true\n",
    "              - mountPath: /sys/fs/cgroup\n",
    "                name: cgroup\n",
    "              - name: dind-storage\n",
    "                mountPath: /var/lib/docker                 \n",
    "```\n",
    "And finally we also mount the docker auth configuration so we don't have to run `docker login`:\n",
    "```\n",
    "              - mountPath: /builder/home/.docker\n",
    "                name: jenkins-docker-config-volume\n",
    "```\n",
    "\n",
    "And to finalise, we need to make sure that the pod can run with privileged context.\n",
    "\n",
    "The reason why this is required is in order to be able to run the docker daemon:\n",
    "```\n",
    "            securityContext:\n",
    "              privileged: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Jenkins X file and testing\n",
    "\n",
    "Now that we've gotten a breakdown of the different additions for the `jenkins-x.yml` file, we can update it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jenkins-x.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile jenkins-x.yml\n",
    "buildPack: none\n",
    "pipelineConfig:\n",
    "  pipelines:\n",
    "    release:\n",
    "      pipeline:\n",
    "        agent:\n",
    "          image: seldonio/core-builder:0.4\n",
    "        stages:\n",
    "        - name: build-and-test\n",
    "          parallel:\n",
    "          - name: test-and-deploy-sklearn-server\n",
    "            steps:\n",
    "            - name: test-sklearn-server\n",
    "              steps:\n",
    "              - name: run-tests\n",
    "                command: make\n",
    "                args:\n",
    "                - install_dev\n",
    "                - test\n",
    "            - name: build-and-push-images\n",
    "              command: bash\n",
    "              args:\n",
    "              - assets/scripts/build_and_push_docker_daemon.sh\n",
    "        options:\n",
    "          containerOptions:\n",
    "            volumeMounts:\n",
    "              - mountPath: /lib/modules\n",
    "                name: modules\n",
    "                readOnly: true\n",
    "              - mountPath: /sys/fs/cgroup\n",
    "                name: cgroup\n",
    "              - name: dind-storage\n",
    "                mountPath: /var/lib/docker\n",
    "              - mountPath: /builder/home/.docker\n",
    "                name: jenkins-docker-config-volume\n",
    "            securityContext:\n",
    "              privileged: true\n",
    "          volumes:\n",
    "            - name: modules\n",
    "              hostPath:\n",
    "                path: /lib/modules\n",
    "                type: Directory\n",
    "            - name: cgroup\n",
    "              hostPath:\n",
    "                path: /sys/fs/cgroup\n",
    "                type: Directory\n",
    "            - name: dind-storage\n",
    "              emptyDir: {}\n",
    "            - name: jenkins-docker-config-volume\n",
    "              secret:\n",
    "                items:\n",
    "                - key: config.json\n",
    "                  path: config.json\n",
    "                secretName: jenkins-docker-cfg\n",
    "    pullRequest:\n",
    "      pipeline:\n",
    "        agent:\n",
    "          image: seldonio/core-builder:0.4\n",
    "        stages:\n",
    "        - name: build-and-test\n",
    "          parallel:\n",
    "          - name: test-and-deploy-sklearn-server\n",
    "            steps:\n",
    "            - name: test-sklearn-server\n",
    "              steps:\n",
    "              - name: run-tests\n",
    "                command: make\n",
    "                args:\n",
    "                - install_dev\n",
    "                - test\n",
    "            - name: build-and-push-images\n",
    "              command: bash\n",
    "              args:\n",
    "              - assets/scripts/build_and_push_docker_daemon.sh\n",
    "        options:\n",
    "          containerOptions:\n",
    "            volumeMounts:\n",
    "              - mountPath: /lib/modules\n",
    "                name: modules\n",
    "                readOnly: true\n",
    "              - mountPath: /sys/fs/cgroup\n",
    "                name: cgroup\n",
    "              - name: dind-storage\n",
    "                mountPath: /var/lib/docker\n",
    "              - mountPath: /builder/home/.docker\n",
    "                name: jenkins-docker-config-volume\n",
    "            securityContext:\n",
    "              privileged: true\n",
    "          volumes:\n",
    "            - name: modules\n",
    "              hostPath:\n",
    "                path: /lib/modules\n",
    "                type: Directory\n",
    "            - name: cgroup\n",
    "              hostPath:\n",
    "                path: /sys/fs/cgroup\n",
    "                type: Directory\n",
    "            - name: dind-storage\n",
    "              emptyDir: {}\n",
    "            - name: jenkins-docker-config-volume\n",
    "              secret:\n",
    "                items:\n",
    "                - key: config.json\n",
    "                  path: config.json\n",
    "                secretName: jenkins-docker-cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model that we want to be able to deploy, we want to make sure that we run end-to-end tests on that model to make sure everything works as expected.\n",
    "\n",
    "For this we will leverage the same framework that the Kubernetes team uses to test Kubernetes itself: KIND.\n",
    "\n",
    "KIND stands for Kubernetes in Docker, and is used to isolate a Kubernetes environent for end-to-end tests.\n",
    "\n",
    "In our case, we will be able to leverage to create an isolated environment, where we'll be able to test our model.\n",
    "\n",
    "For this, the steps we'll have to carry out include:\n",
    "\n",
    "1. Leverage the `kind_run_all.sh` script that creates a KIND cluster and runs the tests\n",
    "2. A simple test that performs integration testing on our model\n",
    "3. The steps in the `Jenkins-X.yml` to run this in the production cluster\n",
    "\n",
    "### Kind run all script\n",
    "\n",
    "The kind_run_all may seem complicated at first, but it's actually quite simple. \n",
    "\n",
    "All the script does is set-up a kind cluster with all dependencies, deploy the model and clean everything up.\n",
    "\n",
    "Let's break down each of the components within the script.\n",
    "\n",
    "#### Start docker\n",
    "\n",
    "We first start the docker daemon and wait until Docker is running (using `docker ps q` for guidance.\n",
    "\n",
    "```\n",
    "# FIRST WE START THE DOCKER DAEMON\n",
    "service docker start\n",
    "# the service can be started but the docker socket not ready, wait for ready\n",
    "WAIT_N=0\n",
    "while true; do\n",
    "    # docker ps -q should only work if the daemon is ready\n",
    "    docker ps -q > /dev/null 2>&1 && break\n",
    "    if [[ ${WAIT_N} -lt 5 ]]; then\n",
    "        WAIT_N=$((WAIT_N+1))\n",
    "        echo \"[SETUP] Waiting for Docker to be ready, sleeping for ${WAIT_N} seconds ...\"\n",
    "        sleep ${WAIT_N}\n",
    "    else\n",
    "        echo \"[SETUP] Reached maximum attempts, not waiting any longer ...\"\n",
    "        break\n",
    "    fi\n",
    "done\n",
    "```\n",
    "\n",
    "#### Create and set-up KIND cluster\n",
    "\n",
    "Once we're running a docker daemon, we can run the command to create our KIND cluster, and install all the components.\n",
    "\n",
    "This will set up a Kubnernetes cluster using the docker daemon (using containers as Nodes), and then install Ambassador + Seldon Core.\n",
    "\n",
    "```\n",
    "#######################################\n",
    "# AVOID EXIT ON ERROR FOR FOLLOWING CMDS\n",
    "set +o errexit\n",
    "\n",
    "# START CLUSTER \n",
    "make kind_create_cluster\n",
    "KIND_EXIT_VALUE=$?\n",
    "\n",
    "# Ensure we reach the kubeconfig path\n",
    "export KUBECONFIG=$(kind get kubeconfig-path)\n",
    "\n",
    "# ONLY RUN THE FOLLOWING IF SUCCESS\n",
    "if [[ ${KIND_EXIT_VALUE} -eq 0 ]]; then\n",
    "    # KIND CLUSTER SETUP\n",
    "    make kind_setup\n",
    "    SETUP_EXIT_VALUE=$?\n",
    "```\n",
    "\n",
    "#### Run python tests\n",
    "\n",
    "We can now run the tests; for this we run all the dev installations and kick off our tests (which we'll add inside of the integration folder).\n",
    "\n",
    "```\n",
    "    # BUILD S2I BASE IMAGES\n",
    "    make build\n",
    "    S2I_EXIT_VALUE=$?\n",
    "\n",
    "    ## INSTALL ALL REQUIRED DEPENDENCIES\n",
    "    make install_integration_dev\n",
    "    INSTALL_EXIT_VALUE=$?\n",
    "    \n",
    "    ## RUNNING TESTS AND CAPTURING ERROR\n",
    "    make test\n",
    "    TEST_EXIT_VALUE=$?\n",
    "fi\n",
    "```\n",
    "\n",
    "#### Clean up\n",
    "\n",
    "Finally we just clean everything, including the cluster, the containers and the docker daemon.\n",
    "\n",
    "```\n",
    "# DELETE KIND CLUSTER\n",
    "make kind_delete_cluster\n",
    "DELETE_EXIT_VALUE=$?\n",
    "\n",
    "#######################################\n",
    "# EXIT STOPS COMMANDS FROM HERE ONWARDS\n",
    "set -o errexit\n",
    "\n",
    "# CLEANING DOCKER\n",
    "docker ps -aq | xargs -r docker rm -f || true\n",
    "service docker stop || true\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write an integration test\n",
    "\n",
    "Now that we have the script that can kick everything off, we need too write a test that would run.\n",
    "\n",
    "For this initial test, we'll write something very simple - it will be the exact test we had previously, but instead of running it by just calling the Python function, we will be sending a POST request using the Python SeldonClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting integration/test_e2e_sklearn_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile integration/test_e2e_sklearn_server.py\n",
    "from seldon_core.seldon_client import SeldonClient\n",
    "import numpy as np\n",
    "\n",
    "API_AMBASSADOR = \"localhost:8003\"\n",
    "\n",
    "def test_sklearn_server():\n",
    "    data = [\"From: brian@ucsd.edu (Brian Kantor)\\nSubject: Re: HELP for Kidney Stones ..............\\nOrganization: The Avant-Garde of the Now, Ltd.\\nLines: 12\\nNNTP-Posting-Host: ucsd.edu\\n\\nAs I recall from my bout with kidney stones, there isn't any\\nmedication that can do anything about them except relieve the pain.\\n\\nEither they pass, or they have to be broken up with sound, or they have\\nto be extracted surgically.\\n\\nWhen I was in, the X-ray tech happened to mention that she'd had kidney\\nstones and children, and the childbirth hurt less.\\n\\nDemerol worked, although I nearly got arrested on my way home when I barfed\\nall over the police car parked just outside the ER.\\n\\t- Brian\\n\",\n",
    "            'From: rind@enterprise.bih.harvard.edu (David Rind)\\nSubject: Re: Candida(yeast) Bloom, Fact or Fiction\\nOrganization: Beth Israel Hospital, Harvard Medical School, Boston Mass., USA\\nLines: 37\\nNNTP-Posting-Host: enterprise.bih.harvard.edu\\n\\nIn article <1993Apr26.103242.1@vms.ocom.okstate.edu>\\n banschbach@vms.ocom.okstate.edu writes:\\n>are in a different class.  The big question seems to be is it reasonable to \\n>use them in patients with GI distress or sinus problems that *could* be due \\n>to candida blooms following the use of broad-spectrum antibiotics?\\n\\nI guess I\\'m still not clear on what the term \"candida bloom\" means,\\nbut certainly it is well known that thrush (superficial candidal\\ninfections on mucous membranes) can occur after antibiotic use.\\nThis has nothing to do with systemic yeast syndrome, the \"quack\"\\ndiagnosis that has been being discussed.\\n\\n\\n>found in the sinus mucus membranes than is candida.  Women have been known \\n>for a very long time to suffer from candida blooms in the vagina and a \\n>women is lucky to find a physician who is willing to treat the cause and \\n>not give give her advise to use the OTC anti-fungal creams.\\n\\nLucky how?  Since a recent article (randomized controlled trial) of\\noral yogurt on reducing vaginal candidiasis, I\\'ve mentioned to a \\nnumber of patients with frequent vaginal yeast infections that they\\ncould try eating 6 ounces of yogurt daily.  It turns out most would\\nrather just use anti-fungal creams when they get yeast infections.\\n\\n>yogurt dangerous).  If this were a standard part of medical practice, as \\n>Gordon R. says it is, then the incidence of GI distress and vaginal yeast \\n>infections should decline.\\n\\nAgain, this just isn\\'t what the systemic yeast syndrome is about, and\\nhas nothing to do with the quack therapies that were being discussed.\\nThere is some evidence that attempts to reinoculate the GI tract with\\nbacteria after antibiotic therapy don\\'t seem to be very helpful in\\nreducing diarrhea, but I don\\'t think anyone would view this as a\\nquack therapy.\\n-- \\nDavid Rind\\nrind@enterprise.bih.harvard.edu\\n']\n",
    "    labels = [2, 2]\n",
    "    \n",
    "    sc = SeldonClient(\n",
    "        gateway=\"ambassador\", \n",
    "        gateway_endpoint=API_AMBASSADOR,\n",
    "        deployment_name=\"news-classifier-server\",\n",
    "        payload_type=\"ndarray\",\n",
    "        namespace=\"default\",\n",
    "        transport=\"rest\")\n",
    "\n",
    "    result = sc.predict(np.array(data))\n",
    "    assert all(result.response.data.ndarray.values == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend JenkinsX file for integration\n",
    "\n",
    "Now that we have the test that would run for the integration tests, we need to extend the JX pipeline to run this.\n",
    "\n",
    "This extension is quite simple, and only requires adding the following line:\n",
    "    \n",
    "```\n",
    "            - name: run-end-to-end-tests\n",
    "              command: bash\n",
    "              args:\n",
    "              - integration/kind_test_all.sh\n",
    "```\n",
    "\n",
    "This line would be added in both the PR and release pipelines so that we can run integration tests then.\n",
    "\n",
    "It is also possible to move the integration tests into a separate jenkins-x file such as `jenkins-x-integration.yml` by leveraging [Contexts & Schedules]() which basically allow us to extend the functionality of Prow by writing our own triggers, however this is outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
